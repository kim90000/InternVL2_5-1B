{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guIU97g3xslB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes -q"
      ],
      "metadata": {
        "id": "Uj4ffPO2wWqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install decord"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "J6wRh7UQwWqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "\n",
        "# Load the model and tokenizer\n",
        "path = \"OpenGVLab/InternVL2_5-1B\"\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_8bit=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True\n",
        ").eval()  # Remove .cuda() here\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
        "\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "1sk7RAUrwWqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from decord import VideoReader, cpu\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Define build_transform and dynamic_preprocess functions\n",
        "def build_transform(input_size=448):\n",
        "    \"\"\"Builds a transformation pipeline for image preprocessing.\"\"\"\n",
        "    transform = T.Compose([\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return transform\n",
        "\n",
        "\n",
        "def dynamic_preprocess(image, image_size=448, use_thumbnail=True, max_num=12):\n",
        "    \"\"\"Dynamically preprocesses the image.\"\"\"\n",
        "    # Placeholder for dynamic preprocessing logic (replace with actual implementation)\n",
        "    # This example simply returns the original image in a list\n",
        "    return [image]\n",
        "\n",
        "# Load the model and tokenizer\n",
        "path = \"OpenGVLab/InternVL2_5-1B\"\n",
        "model = AutoModel.from_pretrained(\n",
        "    path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    load_in_8bit=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_flash_attn=True,\n",
        "    trust_remote_code=True\n",
        ").eval()  # Remove .cuda() here\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
        "\n",
        "\n",
        "def load_image(image_file, input_size=448, max_num=None):  # Parameter max_num set to None.\n",
        "    \"\"\"Loads an image and preprocesses it.\n",
        "    If 'max_num' is not None, it is passed to dynamic_preprocess, otherwise a default of 12 is used.\"\"\"\n",
        "\n",
        "    image = Image.open(image_file).convert('RGB')\n",
        "    transform = build_transform(input_size=input_size)\n",
        "    if max_num:  # Check if max_num is provided\n",
        "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
        "    else:\n",
        "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=12) # Default max_num to 12\n",
        "    pixel_values = [transform(image) for image in images]\n",
        "    pixel_values = torch.stack(pixel_values)\n",
        "    return pixel_values"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2vVA_kkFwWqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "image_path = '/content/1.png'\n",
        "def load_image(image_file, input_size=448, max_num=None):  # Parameter max_num set to None.\n",
        "    \"\"\"Loads an image and preprocesses it.\n",
        "    If 'max_num' is not None, it is passed to dynamic_preprocess, otherwise a default of 12 is used.\"\"\"\n",
        "\n",
        "    image = Image.open(image_file).convert('RGB')\n",
        "    transform = build_transform(input_size=input_size)\n",
        "    if max_num:  # Check if max_num is provided\n",
        "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
        "    else:\n",
        "        images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=12) # Default max_num to 12\n",
        "    pixel_values = [transform(image) for image in images]\n",
        "    pixel_values = torch.stack(pixel_values)\n",
        "    return pixel_values\n",
        "\n",
        "pixel_values = load_image(image_path).to(torch.bfloat16) # Keep pixel_values on CPU\n",
        "generation_config = dict(max_new_tokens=1024, do_sample=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S2f3x2BTwWqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = '<image>\\nPlease describe the image shortly.'\n",
        "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
        "print(f'User: {question}\\nAssistant: {response}')"
      ],
      "metadata": {
        "id": "_gBon5pFwWqN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}